{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":468138,"sourceType":"datasetVersion","datasetId":215646,"isSourceIdPinned":false}],"dockerImageVersionId":31259,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/deepkayastha6890/transaction-risk-pattern-finder?scriptVersionId=297098559\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport kagglehub\nfrom kagglehub import KaggleDatasetAdapter\nimport os\n\n# Addition paths\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Finding the file and assigning the dataframe column","metadata":{}},{"cell_type":"code","source":"file_path = 'bank.xlsx'\n\ndf = kagglehub.load_dataset(\n    KaggleDatasetAdapter.PANDAS,\n    \"apoorvwatsky/bank-transaction-data\",\n    file_path,\n)\n\n# Dataframe first five rows\nprint(\"\\n\\nFirst 5 records:\")\ndisplay(df.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### In this cell cleaning the null data also removing the no needed columns","metadata":{}},{"cell_type":"code","source":"def DataCleaning(df): \n    \n    cleaningData = df.copy()\n\n    #Finding the information \n    print(\"\\n\\nInformations:\\n\",cleaningData.info())\n\n    # Finding the null values\n    majorColumns = ['DEPOSIT AMT','WITHDRAWAL AMT','DATE','VALUE DATE']\n    for i in majorColumns:\n        print(f'\\n\\n{i} : ',cleaningData[i].isnull().sum())\n\n    # Filling the null values\n    nullValuesFiller = ['DEPOSIT AMT','WITHDRAWAL AMT','CHQ.NO.']\n    for i in nullValuesFiller:\n        cleaningData[i] = cleaningData[i].fillna(0)\n\n    # Just changing the date name\n    cleaningData['ActualDate'] = cleaningData['VALUE DATE']\n    \n    # Seperating the day , month , year \n    cleaningData['ActualDay'] = cleaningData['VALUE DATE'].dt.day \n    cleaningData['ActualMonth'] = cleaningData['VALUE DATE'].dt.month\n    cleaningData['ActualYear'] = cleaningData['VALUE DATE'].dt.year\n\n    # Sorting the data from starting date\n    cleaningData = cleaningData.sort_values(by=\"VALUE DATE\", ascending=False)\n\n    # Droping unuseful data colums [Account No, , . ,CHQ.NO. ]\n    cleaningData = cleaningData.drop(columns=['.','CHQ.NO.','VALUE DATE'])\n    \n    return cleaningData\n    \ncleanData = DataCleaning(df)\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def DataDiscribtion(df):\n\n    dataDiscribe = df.copy()\n    # Finding the columns \n    print('\\n')\n    print(dataDiscribe.columns)\n\n    # finding the shape\n    print(\"\\nDataset Size\",dataDiscribe.shape)\n\n    # Finding the data discribtion \n    print('\\nDiscribtion of the data : ')\n    display(dataDiscribe.describe())\n\n    \n    # Year wise count\n    total_year_counts = dataDiscribe['ActualYear'].value_counts()\n\n    print(\"Total list of year : \")\n    display(total_year_counts.head())\n    print(\"\\n\")\n    \n    # Less Then Zero balance\n    count =0\n    for value in dataDiscribe['BALANCE AMT']:\n        if(value < 0):\n            count = count +1;\n    \n    # Counting the minus value\n    print(\"\\n\\nMinus Value in balance : \",count)\n    \n    # Finding the data distribution\n    print('\\n\\nUnique value of the account numbers')\n    display(dataDiscribe['Account No'].unique())\n    \nDataDiscribtion(cleanData)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Actual Dataset have total 10 use full column \n- In the dataset there is multiple account numbers\n- Total rows 116201\n- total 5 years of data is availble","metadata":{}},{"cell_type":"markdown","source":"### Using this two function we can group the dataset","metadata":{}},{"cell_type":"code","source":"# Count of compressed features\ndef groupByCounting(df, respectedMainFeature, copresstionFeatures):\n    return (\n        df\n        .groupby(respectedMainFeature)[copresstionFeatures]\n        .count()\n        .sort_index()\n    )\n# Sum of the compress features\ndef groupBySum(df, respectedMainFeature, copresstionFeatures):\n    return (\n        df\n        .groupby(respectedMainFeature)[copresstionFeatures]\n        .sum()\n        .sort_index()\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# This is the main feature engineering portion using this we have the risk factors and risk of the bank account","metadata":{}},{"cell_type":"code","source":"# Day wise data process and account No wise classifier\ndef build_daily_account_log(df):\n\n    # Day wise agrigated data and this data is days wise\n    daily_log = (\n        df\n        .sort_values(['Account No', 'DATE'])\n        .groupby(['Account No', 'DATE'])\n        .agg(\n            TotalDeposit=('DEPOSIT AMT', 'sum'),\n            TotalWithdrawal=('WITHDRAWAL AMT', 'sum'),\n            EndBalance=('BALANCE AMT', 'last'),\n            TxnCount=('BALANCE AMT', 'count')\n        )\n        .reset_index()\n    )\n\n    # Net movement for the day\n    daily_log['NetChange'] = (\n        daily_log['TotalDeposit'] - daily_log['TotalWithdrawal']\n    )\n\n    # This making diffrent dataframes \n    daily_account_df = daily_log\n\n\n    # Now daily_account_df is new all in one data frame\n    # If overdraf this goes true\n    daily_account_df['IsOverdraft'] = daily_account_df['EndBalance'] < 0\n    \n    # Marking overdraft\n    daily_account_df['OverdraftAmount'] = (\n        -daily_account_df['EndBalance']\n    ).clip(lower=0)\n\n    # Daily balance change\n    daily_account_df['DailyBalanceChange'] = (\n    daily_account_df\n        .groupby('Account No')['EndBalance']\n        .diff()\n    )\n    daily_account_df['MONTH'] = daily_account_df['DATE'].dt.to_period('M')\n\n    # Two dataframes are there\n    return daily_log , daily_account_df\n_ , manngedData = build_daily_account_log(cleanData)\n\ndisplay(manngedData.shape)\ndisplay(manngedData.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def monthly_account_features(df):\n\n    df = df.sort_values(['Account No', 'DATE']).copy()\n\n    # putting the feature risk one row behind\n    df['StreakStart'] = (\n        (df['IsOverdraft']) &\n        (~df.groupby('Account No')['IsOverdraft'].shift(fill_value=False))\n    )\n\n    # Assigning the streak id\n    df['StreakID'] = df.groupby('Account No')['StreakStart'].cumsum()\n\n    # Counting the overdraft day\n    overdraft_days = df[df['IsOverdraft']]\n\n    \n    streaks = (\n        overdraft_days\n        .groupby(['Account No', 'MONTH', 'StreakID'])\n        .size()\n        .reset_index(name='StreakLength')\n    )\n\n    # Avg overdraft streak per month (Recovery Velocity)\n    recovery_velocity = (\n        streaks\n        .groupby(['Account No', 'MONTH'])['StreakLength']\n        .mean()\n        .reset_index(name='RecoveryVelocity')\n    )\n\n    # MONTHLY grouping the data \n    monthly = (\n        df\n        .groupby(['Account No', 'MONTH'])\n        .agg(\n            DaysInMonth=('DATE', 'nunique'),\n            StressDays=('IsOverdraft', 'sum'),\n            NetCashflow=('NetChange', 'sum'),\n            TotalTxn=('TxnCount', 'sum'),\n            MaxWithdrawal=('TotalWithdrawal', 'max'),\n            AvgEndBalance=('EndBalance', 'mean'),\n            BalanceVolatility=('EndBalance', 'std')\n        )\n        .reset_index()\n    )\n\n    # 2. Whale Impact\n    monthly['WhaleImpact'] = (\n        monthly['MaxWithdrawal'] /\n        monthly['AvgEndBalance'].abs()\n    )\n\n    # 3. Stress Duration\n    monthly['StressDuration'] = (\n        monthly['StressDays'] / monthly['DaysInMonth']\n    )\n\n    # 5. Operational Intensity\n    monthly['OperationalIntensity'] = (\n        monthly['TotalTxn'] / monthly['DaysInMonth']\n    )\n\n    # MERGE RECOVERY VELOCITY\n    final_features = monthly.merge(\n        recovery_velocity,\n        on=['Account No', 'MONTH'],\n        how='left'\n    )\n\n    final_features['RecoveryVelocity'] = (\n        final_features['RecoveryVelocity'].fillna(0)\n    )\n\n    return final_features\nmonthly_features = monthly_account_features(manngedData)\ndisplay(monthly_features.head(10))\nmonthly_features.tail(10)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def modelDataFrame(df):\n    df = df.copy()\n    \n    df['Norm_Recovery'] = (df['RecoveryVelocity'] / 30.0).clip(upper=1.0)\n    df['Norm_Stress'] = df['StressDuration'].clip(upper=1.0)\n    df['Norm_Whale'] = df['WhaleImpact'].clip(upper=1.0)\n    operating_scale = df[['MaxWithdrawal', 'AvgEndBalance']].abs().max(axis=1)\n    df['Norm_Cashflow'] = np.where(\n        df['NetCashflow'] >= 0,\n        0.0,\n        (df['NetCashflow'].abs() / operating_scale).clip(upper=1.0)\n    )\n    df['Norm_Volatility'] = (\n        np.log1p(df['BalanceVolatility']) / np.log1p(df['AvgEndBalance'].abs())\n    ).clip(upper=1.0)\n    df['Norm_Intensity'] = (df['OperationalIntensity'] / 50.0).clip(upper=1.0)\n    relation = df[\n        ['Norm_Recovery', 'Norm_Stress', 'Norm_Cashflow',\n         'Norm_Whale', 'Norm_Intensity', 'Norm_Volatility']\n    ].copy()\n    df['Risk_Score'] = relation.mean(axis=1) * 100\n\n    return df.round(2), relation.round(3)\n\n\n# Execution\nmodelData, relation = modelDataFrame(monthly_features)\n\nmodelData = modelData.sort_values(['Account No', 'MONTH'])\nmodelData['FutureRisk'] = modelData.groupby('Account No')['Risk_Score'].shift(-1)\n\ntrain_df = modelData.dropna(subset=['FutureRisk'])\nlive_df  = modelData[modelData['FutureRisk'].isna()]\nrelation_corr = relation.copy()\nrelation_corr['risk'] = modelData['FutureRisk']\n\ncorr_matrix = relation_corr.corr(method='spearman')\nprint(corr_matrix)\nmodelData.to_csv('myData.csv')\nmodelData.describe ()\n\nfinalData = modelData[\n    ['Norm_Recovery', 'Norm_Stress', 'Norm_Cashflow',\n     'Norm_Whale', 'Norm_Intensity', 'Norm_Volatility','FutureRisk',\n     'Account No','MONTH']  \n].copy()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.dates as mdates\n\nif pd.api.types.is_period_dtype(train_df['MONTH']):\n    train_df['MONTH'] = train_df['MONTH'].dt.to_timestamp()\nelse:\n    if not pd.api.types.is_datetime64_any_dtype(train_df['MONTH']):\n        train_df['MONTH'] = pd.to_datetime(train_df['MONTH'])\n\ntrain_df['Account No'] = train_df['Account No'].astype(str)\n\nsns.set_theme(style=\"whitegrid\")\naccounts = train_df['Account No'].unique()\n\nplt.figure(figsize=(14, 6))\nrisk_melt = train_df.melt(id_vars=['Account No', 'MONTH'], \n                          value_vars=['Risk_Score', 'FutureRisk'], \n                          var_name='Risk_Type', value_name='Score')\n\nsns.lineplot(data=risk_melt, x='MONTH', y='Score', hue='Account No', style='Risk_Type', markers=True, dashes=False)\nplt.title('Risk Trajectory: Current vs. Predicted Risk (All Accounts)', fontsize=16)\nplt.ylabel('Risk Score')\nplt.xlabel('Date')\nplt.legend(title='Account / Risk Type')\nplt.tight_layout()\nplt.show()\n\n# account wise net cashflow and risk score\nfor acc in accounts:\n    subset = train_df[train_df['Account No'] == acc]\n    \n    fig, ax1 = plt.subplots(figsize=(14, 6))\n    \n    color_cash = 'tab:green'\n    ax1.set_xlabel('Month')\n    ax1.set_ylabel('Net Cashflow', color=color_cash, fontweight='bold')\n    sns.lineplot(data=subset, x='MONTH', y='NetCashflow', ax=ax1, color=color_cash, marker='o', linewidth=2.5)\n    ax1.tick_params(axis='y', labelcolor=color_cash)\n    ax1.axhline(0, color='black', linestyle='--', linewidth=1) # Zero line\n    \n    ax2 = ax1.twinx()\n    color_stress = 'tab:red'\n    ax2.set_ylabel('Stress Days (Count)', color=color_stress, fontweight='bold')\n    \n    width = 20 \n    ax2.bar(subset['MONTH'], subset['StressDays'], color=color_stress, alpha=0.3, width=20, label='Stress Days')\n    ax2.tick_params(axis='y', labelcolor=color_stress)\n    \n    \n    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n    plt.title(f'Account Status: {acc} (Cashflow vs Stress)', fontsize=16, weight='bold')\n    \n    latest = subset.iloc[-1]\n    status_text = \"STABLE\" if latest['Risk_Score'] < 50 else \"AT RISK\"\n    status_color = \"green\" if status_text == \"STABLE\" else \"red\"\n    \n    plt.figtext(0.15, 0.8, f\"Current Status: {status_text}\\nRisk Score: {latest['Risk_Score']:.2f}\", \n                bbox=dict(facecolor=status_color, alpha=0.2), fontsize=12)\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n\ndef train_and_predict_future_risk(finalData):\n    df = finalData.copy()\n\n    base_features = [\n        'Norm_Recovery',\n        'Norm_Stress',\n        'Norm_Cashflow',\n        'Norm_Whale',\n        'Norm_Intensity',\n        'Norm_Volatility'\n    ]\n\n    needed_cols = base_features + ['FutureRisk', 'Account No', 'MONTH']\n    for col in needed_cols:\n        if col not in df.columns:\n            raise ValueError(f\"Missing column in finalData: {col}\")\n\n    df = df.dropna(subset=base_features).copy()\n\n    df = df.sort_values(['Account No', 'MONTH']).copy()\n\n    for col in base_features:\n        df[f\"{col}_lag1\"] = df.groupby(\"Account No\")[col].shift(1)\n        df[f\"{col}_lag2\"] = df.groupby(\"Account No\")[col].shift(2)\n\n    lag_cols = [f\"{c}_lag1\" for c in base_features] + [f\"{c}_lag2\" for c in base_features]\n\n    df_train = df.dropna(subset=['FutureRisk'] + lag_cols).copy()\n\n    feature_cols = base_features + lag_cols\n\n    split_point = df_train['MONTH'].quantile(0.8)\n\n    train_part = df_train[df_train['MONTH'] <= split_point]\n    val_part   = df_train[df_train['MONTH'] > split_point]\n\n    X_train = train_part[feature_cols]\n    y_train = train_part['FutureRisk']\n\n    X_val = val_part[feature_cols]\n    y_val = val_part['FutureRisk']\n\n    model = XGBRegressor(\n        n_estimators=2000,\n        learning_rate=0.02,\n        max_depth=4,\n        subsample=0.85,\n        colsample_bytree=0.85,\n        min_child_weight=3,\n        reg_alpha=0.2,\n        reg_lambda=1.5,\n        objective=\"reg:squarederror\",\n        random_state=42\n    )\n\n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_val, y_val)],\n        verbose=False\n    )\n\n    val_preds = model.predict(X_val)\n\n    metrics = {\n        \"MAE\": mean_absolute_error(y_val, val_preds),\n        \"RMSE\": np.sqrt(mean_squared_error(y_val, val_preds)),\n        \"R2\": r2_score(y_val, val_preds)\n    }\n\n    print(\"Metrics:\", metrics)\n\n    plt.figure(figsize=(6,6))\n    plt.scatter(y_val, val_preds, alpha=0.5)\n    plt.plot([y_val.min(), y_val.max()],\n             [y_val.min(), y_val.max()],\n             linestyle=\"--\")\n    plt.xlabel(\"Actual Future Risk\")\n    plt.ylabel(\"Predicted Future Risk\")\n    plt.title(\"Predicted vs Actual Future Risk (Validation)\")\n    plt.show()\n\n    residuals = y_val - val_preds\n\n    plt.figure(figsize=(6,4))\n    plt.scatter(val_preds, residuals, alpha=0.5)\n    plt.axhline(0, linestyle=\"--\")\n    plt.xlabel(\"Predicted Future Risk\")\n    plt.ylabel(\"Residual (Actual âˆ’ Predicted)\")\n    plt.title(\"Residuals vs Predicted Risk\")\n    plt.show()\n\n    # Feature importance\n    importance = pd.Series(model.feature_importances_, index=feature_cols).sort_values()\n\n    plt.figure(figsize=(7,5))\n    importance.tail(20).plot(kind=\"barh\")  \n    plt.xlabel(\"Importance\")\n    plt.title(\"Top Feature Importance (XGBoost)\")\n    plt.show()\n\n    df_live = df.copy()\n\n    df_live = df_live.dropna(subset=lag_cols).copy()\n\n    # Latest month per account\n    df_live_latest = (\n        df_live.sort_values(['Account No', 'MONTH'])\n        .groupby(\"Account No\")\n        .tail(1)\n        .copy()\n    )\n\n    df_live_latest[\"PredictedFutureRisk\"] = model.predict(df_live_latest[feature_cols])\n\n    live_predictions = df_live_latest[\n        [\"Account No\", \"MONTH\", \"PredictedFutureRisk\"]\n    ].reset_index(drop=True)\n\n    return model, metrics, live_predictions\n\nmodel, metrics, predictions = train_and_predict_future_risk(finalData)\n\nprint(metrics)\ndisplay(predictions.head(10))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}